{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRABAJO FINAL INTEGRADOR\n",
    "\n",
    "### NOMBRE: GUIDO PETTINARI\n",
    "\n",
    "### TUTOR: VALERIA SOLIANI\n",
    "\n",
    "### UNIVERSIDAD: INSTITUTO TECNOLOGICO DE BUENOS AIRES (ITBA)\n",
    "\n",
    "**El Trabajo se dividirá en**:\n",
    "\n",
    "    1) Análisis Exploratorio Descriptivo\n",
    "    2) Feature Engineering o Preparación de Datos\n",
    "    3) Modelos Predictivos: Probaremos diferentes algoritmos predictivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score,make_scorer,f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mlxtend.evaluate import lift_score\n",
    "import csv\n",
    "from funpymodeling.exploratory import status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = pd.read_csv('data_base.csv')\n",
    "data_ohe = pd.read_csv('data_ohe.csv')\n",
    "data_ohe_corr = pd.read_csv('data_ohe_corr.csv')\n",
    "data_ohe_out = pd.read_csv('data_ohe_out.csv')\n",
    "data_ohe_corr_out = pd.read_csv('data_ohe_out_corr.csv')\n",
    "data_out = pd.read_csv('data_out.csv')\n",
    "data_corr = pd.read_csv('data_corr.csv')\n",
    "data_out_corr = pd.read_csv('data_out_corr.csv')\n",
    "data_index = pd.read_csv('data_index.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split All DF in Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_X = data_base.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_base_y = data_base[['MonthlyIncome_cat_encode']]\n",
    "data_ohe_X = data_ohe.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_ohe_y = data_ohe[['MonthlyIncome_cat_encode']]\n",
    "data_ohe_corr_X = data_ohe_corr.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_ohe_corr_y = data_ohe_corr[['MonthlyIncome_cat_encode']]\n",
    "data_ohe_out_X = data_ohe_out.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_ohe_out_y = data_ohe_out[['MonthlyIncome_cat_encode']]\n",
    "data_ohe_corr_out_X = data_ohe_corr_out.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_ohe_corr_out_y = data_ohe_corr_out[['MonthlyIncome_cat_encode']]\n",
    "data_out_X = data_out.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_out_y = data_out[['MonthlyIncome_cat_encode']]\n",
    "data_corr_X = data_corr.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_corr_y = data_corr[['MonthlyIncome_cat_encode']]\n",
    "data_out_corr_X = data_out_corr.drop(['MonthlyIncome_cat_encode'],axis=1)\n",
    "data_out_corr_y = data_out_corr[['MonthlyIncome_cat_encode']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_base\n",
    "\n",
    "data_base_X_train, data_base_X_test, data_base_y_train, \\\n",
    "                                     data_base_y_test = train_test_split(data_base_X, data_base_y, test_size=0.3, random_state=42)\n",
    "\n",
    "#data_ohe\n",
    "data_ohe_X_train, data_ohe_X_test, data_ohe_y_train, \\\n",
    "                                   data_ohe_y_test = train_test_split(data_ohe_X, data_ohe_y, test_size=0.3, random_state=42)\n",
    "\n",
    "#data_ohe_corr\n",
    "data_ohe_corr_X_train, data_ohe_corr_X_test, data_ohe_corr_y_train, \\\n",
    "                       data_ohe_corr_y_test = train_test_split(data_ohe_corr_X, data_ohe_corr_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# data_ohe_out\n",
    "data_ohe_out_X_train, data_ohe_out_X_test, data_ohe_out_y_train, \\\n",
    "                      data_ohe_out_y_test = train_test_split(data_ohe_out_X, data_ohe_out_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# data_ohe_corr_out\n",
    "data_ohe_corr_out_X_train, data_ohe_corr_out_X_test, data_ohe_corr_out_y_train, \\\n",
    "                           data_ohe_corr_out_y_test = train_test_split(data_ohe_corr_out_X, data_ohe_corr_out_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# data_out\n",
    "data_out_X_train, data_out_X_test, data_out_y_train, \\\n",
    "                           data_out_y_test = train_test_split(data_out_X, data_out_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# data_corr\n",
    "data_corr_X_train, data_corr_X_test, data_corr_y_train, \\\n",
    "                           data_corr_y_test = train_test_split(data_corr_X, data_corr_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# data_out_corr\n",
    "data_out_corr_X_train, data_out_corr_X_test, data_out_corr_y_train, \\\n",
    "                           data_out_corr_y_test = train_test_split(data_out_corr_X, data_out_corr_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "#scaler.fit(data_base_X_train)\n",
    "#data_base_X_train = scaler.transform(data_base_X_train)\n",
    "#data_base_X_test = scaler.transform(data_base_X_test)\n",
    "\n",
    "\n",
    "scaler.fit(data_ohe_X_train)\n",
    "data_ohe_X_train = scaler.transform(data_ohe_X_train)\n",
    "data_ohe_X_test = scaler.transform(data_ohe_X_test)\n",
    "\n",
    "\n",
    "scaler.fit(data_ohe_corr_X_train)\n",
    "data_ohe_corr_X_train = scaler.transform(data_ohe_corr_X_train)\n",
    "data_ohe_corr_X_test = scaler.transform(data_ohe_corr_X_test)\n",
    "\n",
    "\n",
    "scaler.fit(data_ohe_out_X_train)\n",
    "data_ohe_out_X_train = scaler.transform(data_ohe_out_X_train)\n",
    "data_ohe_out_X_test = scaler.transform(data_ohe_out_X_test)\n",
    "\n",
    "scaler.fit(data_ohe_corr_out_X_train)\n",
    "data_ohe_corr_out_X_train = scaler.transform(data_ohe_corr_out_X_train)\n",
    "data_ohe_corr_out_X_test = scaler.transform(data_ohe_corr_out_X_test)\n",
    "\n",
    "\n",
    "#scaler.fit(data_out_X_train)\n",
    "#data_out_X_train = scaler.transform(data_out_X_train)\n",
    "#data_out_X_test = scaler.transform(data_out_X_test)\n",
    "\n",
    "#scaler.fit(data_corr_X_train)\n",
    "#data_corr_X_train = scaler.transform(data_corr_X_train)\n",
    "#data_corr_X_test = scaler.transform(data_corr_X_test)\n",
    "\n",
    "#scaler.fit(data_out_corr_X_train)\n",
    "#data_out_corr_X_train = scaler.transform(data_out_corr_X_train)\n",
    "#data_out_corr_X_test = scaler.transform(data_out_corr_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_leaves': [10, 20, 40, 60, 100], 'max_depth': [5, 10, 20, 30, 40], 'learning_rate': [0.5], 'min_data_in_leaf': [4, 8, 12, 20], 'lambda_l1': [0, 1, 2, 5, 8], 'n_estimators': [10, 30, 60, 120]}\n"
     ]
    }
   ],
   "source": [
    "num_leaves=[10, 20,40,60,100]\n",
    "max_depth=[5,10,20,30,40]\n",
    "learning_rate=[0.5]\n",
    "lambda_l1= [0, 1, 2,5,8]\n",
    "min_data_in_leaf=[4,8,12,20]\n",
    "n_estimators=[10,30,60,120]\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'num_leaves': num_leaves,\n",
    "               'max_depth': max_depth,\n",
    "               'learning_rate': learning_rate,\n",
    "               'min_data_in_leaf': min_data_in_leaf,\n",
    "               'lambda_l1': lambda_l1,\n",
    "               'n_estimators': n_estimators}\n",
    "print(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "scoring = {'lift':make_scorer(lift_score),\n",
    "           'precision': make_scorer(precision_score, average = 'macro'),\n",
    "           'f1_score':make_scorer(f1_score, average='macro'),\n",
    "           'accuracy': make_scorer(accuracy_score),\n",
    "           'recall':make_scorer(recall_score,average='macro'),\n",
    "           'roc_auc_ovr':'roc_auc_ovr'}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = lightgbm.LGBMClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = GridSearchCV(estimator = rf, \n",
    "                               param_grid = random_grid,\n",
    "                               scoring = scoring,\n",
    "                               refit='accuracy',\n",
    "                               cv = 5, \n",
    "                               verbose=1, \n",
    "                               n_jobs = -1)\n",
    "\n",
    "\n",
    "# list dataframes train that Random Forest accepts\n",
    "list_dataframes_train = [#[data_base_X_train, data_base_X_test],\n",
    "                         [data_ohe_X_train, data_ohe_y_train],\n",
    "                         [data_ohe_corr_X_train, data_ohe_corr_y_train],\n",
    "                         [data_ohe_out_X_train, data_ohe_out_y_train],\n",
    "                         [data_ohe_corr_out_X_train, data_ohe_corr_out_y_train]\n",
    "                         #[data_out_X, data_out_y],\n",
    "                         #[data_corr_X, data_corr_y],\n",
    "                         #[data_out_corr_X, data_out_corr_y]\n",
    "                         ]\n",
    "\n",
    "# list dataframes train that Random Forest accepts\n",
    "list_dataframes_test = [#[data_base_X_train, data_base_X_test],\n",
    "                         [data_ohe_X_test, data_ohe_y_test],\n",
    "                         [data_ohe_corr_X_test, data_ohe_corr_y_test],\n",
    "                         [data_ohe_out_X_test, data_ohe_out_y_test],\n",
    "                         [data_ohe_corr_out_X_test, data_ohe_corr_out_y_test]\n",
    "                         #[data_out_X, data_out_y],\n",
    "                         #[data_corr_X, data_corr_y],\n",
    "                         #[data_out_corr_X, data_out_corr_y]\n",
    "                         ]\n",
    "\n",
    "\n",
    "list_dataframes_names = ['data_ohe','data_ohe_corr','data_ohe_out','data_ohe_corr_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] lambda_l1 is set=8, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8\n",
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 474 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 824 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1274 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1824 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2474 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3224 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4074 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5024 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6074 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7224 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 8474 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9824 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 474 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 824 tasks      | elapsed:   58.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1274 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1824 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2474 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3224 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4074 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5024 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6074 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7224 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8474 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9824 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 490 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 840 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1290 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1840 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2490 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3240 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4090 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5040 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6090 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7240 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8490 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  5.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the random search model\n",
    "df = pd.DataFrame()\n",
    "for i in range(0,len(list_dataframes_train)):\n",
    "    #Entreno sobre mis dataframes train  \n",
    "    rf_random.fit(list_dataframes_train[i][0], list_dataframes_train[i][1].values.ravel())\n",
    "    len_name = len(rf_random.cv_results_['mean_test_accuracy'])\n",
    "    name = [list_dataframes_names[i]]*len_name\n",
    "\n",
    "    train_results = pd.concat([\n",
    "                     pd.DataFrame(name,columns=['dataframe']),\n",
    "                     pd.DataFrame(rf_random.cv_results_['params']),\n",
    "                     pd.DataFrame(rf_random.cv_results_['mean_test_lift'],columns=['lift']),          \n",
    "                     pd.DataFrame(rf_random.cv_results_['mean_test_accuracy'],columns=['accuracy']),\n",
    "                     pd.DataFrame(rf_random.cv_results_['mean_test_precision'],columns=['precision_score']),\n",
    "                     pd.DataFrame(rf_random.cv_results_['mean_test_recall'],columns=['recall']),\n",
    "                     pd.DataFrame(rf_random.cv_results_['mean_test_f1_score'],columns=['f1_score']),\n",
    "                     pd.DataFrame(rf_random.cv_results_['mean_test_roc_auc_ovr'],columns=['roc_auc_ovr'])],axis=1).sort_values('lift',ascending=False)\n",
    "    df = df.append(train_results) \n",
    "df.to_csv('train_lightgbm_results_scaler_v4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación correremos con los parametros que nos dió mejor resultado tanto el training como el testing y los graficaremos \n",
    "# con una curva de ROC\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "\n",
    "rf = lightgbm.LGBMClassifier(num_leaves=100,\n",
    "                             max_depth=5,\n",
    "                             learning_rate=[0.5],\n",
    "                             lambda_l1= 8,\n",
    "                             min_data_in_leaf=12,\n",
    "                             n_estimators=60)\n",
    "\n",
    "\n",
    "rf.fit(data_ohe_X_train, data_ohe_y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_tr = rf.predict(data_ohe_X_train)\n",
    "y_pred_ts = rf.predict(data_ohe_X_test)\n",
    "\n",
    "\n",
    "print(classification_report(data_ohe_y_train,y_pred_tr))\n",
    "print(classification_report(data_ohe_y_test,y_pred_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con el mejor parametro del grid search paso a predecir\n",
    "#y_pred_tr = rf_random.predict(X_train)\n",
    "#accuracy_score = accuracy_score(y_train, y_pred_tr)\n",
    "#precision_score = precision_score(y_train, y_pred_tr)\n",
    "#recall_score = recall_score(y_train, y_pred_tr)\n",
    "#roc_auc_score = roc_auc_score(y_train, y_pred_tr)\n",
    "#lift_score = lift_score(y_train, y_pred_tr)\n",
    "\n",
    "pd.unique(data_ohe_y_train['MonthlyIncome_cat_encode'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC,PrecisionRecallCurve,ClassBalance\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "\n",
    "visualizer = ROCAUC(rf, classes=[\"0\",\"1\",\"2\",\"3\"])\n",
    "\n",
    "visualizer.fit(data_ohe_X_train,data_ohe_y_train)\n",
    "visualizer.score(data_ohe_X_test,data_ohe_y_test)\n",
    "visualizer.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
